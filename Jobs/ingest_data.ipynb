{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka Broker/topic detail\n",
    "KAFKA_TOPIC_NAME_CONS = \"users_created\"\n",
    "KAFKA_BOOTSTRAP_SERVERS_CONS = 'localhost:29092' #'localhost:9092'\n",
    "mongoURL = 'mongodb://mongadmin:mongadmin@localhost:27017/UserData.User' #local.startup_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/02 08:29:47 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:29:48 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:29:49 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Real-Time Streaming Data Pipeline\")\\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.2,\"\n",
    "                \"org.mongodb.spark:mongo-spark-connector_2.13:10.3.0\")\\\n",
    "        .config(\"spark.mongodb.read.connection.uri\", mongoURL) \\\n",
    "        .config(\"spark.mongodb.write.connection.uri\", mongoURL) \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/02 08:29:50 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker: nodename nor servname provided, or not known\n",
      "\tat java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:933)\n",
      "\tat java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543)\n",
      "\tat java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:852)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n"
     ]
    }
   ],
   "source": [
    "liveData = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS_CONS) \\\n",
    "    .option(\"subscribe\", KAFKA_TOPIC_NAME_CONS) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[id: string, first_name: string, last_name: string, gender: string, address: string, post_code: string, email: string, username: string, registered_date: string, phone: string, picture: string]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/02 08:32:10 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/zg/17tj2_zj54757r6qn01155p40000gn/T/temporary-ad4f2cde-0430-45f4-a54b-29b3664a7bce. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/07/02 08:32:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/02 08:32:10 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 2147483646 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:291)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.tryConnect(ConsumerNetworkClient.java:572)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:757)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:737)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:599)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:409)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:212)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:230)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:10 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:291)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:212)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:230)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:10 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:10 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:10 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:10 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:11 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:11 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:11 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:11 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:11 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:12 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:12 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:12 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:13 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:13 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:13 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:13 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:13 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:14 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:14 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:14 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker: nodename nor servname provided, or not known\n",
      "\tat java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:933)\n",
      "\tat java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543)\n",
      "\tat java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:852)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:14 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:15 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:15 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:15 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:16 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:16 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:16 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:16 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:16 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:17 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:17 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:18 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:18 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:18 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:18 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:19 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:19 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:19 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:19 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:20 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:20 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:20 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:20 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:21 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:21 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:21 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:21 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:22 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:22 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:22 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:22 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:23 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:23 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:23 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:23 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:24 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:24 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:24 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:24 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:24 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker: nodename nor servname provided, or not known\n",
      "\tat java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:933)\n",
      "\tat java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543)\n",
      "\tat java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:852)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:25 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:25 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:25 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:26 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:26 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:26 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:26 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:27 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:27 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:27 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:27 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:28 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:28 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:28 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:28 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:29 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:29 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:29 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:29 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:30 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:30 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:30 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:30 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:31 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:31 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:31 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:31 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:32 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:32 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:32 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:32 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:33 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:33 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:33 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:33 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:34 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:34 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:34 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:34 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:35 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker: nodename nor servname provided, or not known\n",
      "\tat java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:933)\n",
      "\tat java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543)\n",
      "\tat java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:852)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:35 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:35 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:35 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:36 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:36 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:36 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:36 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:37 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:37 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:37 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:37 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:38 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:38 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:38 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:38 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:39 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:39 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:39 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:39 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:40 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:40 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:40 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:41 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:41 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:41 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:41 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:42 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:42 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:42 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:42 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:43 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:43 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:43 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:43 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:44 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:44 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:44 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:44 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:45 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:45 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:45 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker: nodename nor servname provided, or not known\n",
      "\tat java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:933)\n",
      "\tat java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543)\n",
      "\tat java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:852)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:45 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:46 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:46 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:46 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:46 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:46 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:47 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:47 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:47 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:47 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:48 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:48 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0-7, groupId=spark-kafka-source-e58312f4-7bd9-4d91-bb9c-5ac7fc59534e-299587561-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:48 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:49 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n"
     ]
    }
   ],
   "source": [
    "def create_selection_df_from_kafka(spark_df):\n",
    "    schema = StructType([\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"first_name\", StringType(), False),\n",
    "        StructField(\"last_name\", StringType(), False),\n",
    "        StructField(\"gender\", StringType(), False),\n",
    "        StructField(\"address\", StringType(), False),\n",
    "        StructField(\"post_code\", StringType(), False),\n",
    "        StructField(\"email\", StringType(), False),\n",
    "        StructField(\"username\", StringType(), False),\n",
    "        StructField(\"registered_date\", StringType(), False),\n",
    "        StructField(\"phone\", StringType(), False),\n",
    "        StructField(\"picture\", StringType(), False)\n",
    "    ])\n",
    "\n",
    "    sel = spark_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "        .select(from_json(col('value'), schema).alias('data')).select(\"data.*\")\n",
    "    print(sel)\n",
    "\n",
    "    return sel\n",
    "    # liveData = liveData.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "liveData = create_selection_df_from_kafka(liveData)\n",
    "\n",
    "\n",
    "    \n",
    "# Write the streaming data to the console\n",
    "query = liveData.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/02 08:30:16 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:16 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/zg/17tj2_zj54757r6qn01155p40000gn/T/temporary-1a725601-2ce9-4203-af2d-e53e472531ad. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/07/02 08:30:16 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/07/02 08:30:16 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 2147483646 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:291)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.tryConnect(ConsumerNetworkClient.java:572)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:757)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:737)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:599)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:409)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:212)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:230)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:17 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:291)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:212)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:230)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:17 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:17 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:17 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:17 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:17 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:18 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:18 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:18 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:18 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:19 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:19 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:19 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:20 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:20 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:20 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:21 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:21 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:21 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:22 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker: nodename nor servname provided, or not known\n",
      "\tat java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:933)\n",
      "\tat java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543)\n",
      "\tat java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:852)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:22 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:22 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:23 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:23 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:23 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:24 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:24 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:24 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:25 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:25 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:25 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:26 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:27 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:27 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:27 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:27 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:28 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:28 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:28 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:29 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:29 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:29 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:30 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:30 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:30 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:31 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:31 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:31 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:32 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker: nodename nor servname provided, or not known\n",
      "\tat java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:933)\n",
      "\tat java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543)\n",
      "\tat java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:852)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:32 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:32 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:33 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:33 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:33 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:34 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:34 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:34 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:35 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:35 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:35 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:36 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:36 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:37 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:37 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:38 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:38 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:38 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:39 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:39 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:39 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:40 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:40 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:40 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:40 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:41 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:41 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:42 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:42 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:42 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker: nodename nor servname provided, or not known\n",
      "\tat java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:933)\n",
      "\tat java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543)\n",
      "\tat java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:852)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:43 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:43 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:43 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:44 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:44 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:44 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:45 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:45 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:45 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:45 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:46 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:46 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:47 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:47 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:47 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:48 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:48 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:48 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:49 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:49 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:49 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:50 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:50 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:50 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:51 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:51 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:51 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:52 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:53 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:887)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:53 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker: nodename nor servname provided, or not known\n",
      "\tat java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:933)\n",
      "\tat java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543)\n",
      "\tat java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:852)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:53 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:54 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:54 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:54 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:55 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:55 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:55 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:56 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:56 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:56 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:57 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:57 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:57 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:57 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:58 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:58 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:58 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:59 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:59 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:30:59 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:00 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:00 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:00 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:01 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:01 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:01 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:02 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:02 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:03 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:03 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker: nodename nor servname provided, or not known\n",
      "\tat java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:933)\n",
      "\tat java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543)\n",
      "\tat java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:852)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:04 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:04 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:04 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:05 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:05 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:05 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:06 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:06 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:06 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:07 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:07 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:07 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:08 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:08 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:08 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:08 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:09 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:09 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:09 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:10 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:10 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:10 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:11 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:11 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:11 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:12 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:12 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:12 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:13 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:13 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:13 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker: nodename nor servname provided, or not known\n",
      "\tat java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:933)\n",
      "\tat java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543)\n",
      "\tat java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:852)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:14 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:14 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:14 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:15 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:15 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:15 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/garyhsu/Library/Caches/pypoetry/virtualenvs/realtime-data-streaming-Za3P1rTN-py3.12/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/garyhsu/Library/Caches/pypoetry/virtualenvs/realtime-data-streaming-Za3P1rTN-py3.12/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py\", line 708, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#com.mongodb.spark.sql.DefaultSource\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mliveData\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeachBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite_mongo_row\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/realtime-data-streaming-Za3P1rTN-py3.12/lib/python3.12/site-packages/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/realtime-data-streaming-Za3P1rTN-py3.12/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/realtime-data-streaming-Za3P1rTN-py3.12/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/realtime-data-streaming-Za3P1rTN-py3.12/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/02 08:31:16 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:16 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:16 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:17 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:17 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:17 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:18 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:18 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:18 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:19 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:19 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:20 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:20 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:20 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:20 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:21 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:21 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:21 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:22 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:22 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:22 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:23 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:23 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:23 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker: nodename nor servname provided, or not known\n",
      "\tat java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:933)\n",
      "\tat java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543)\n",
      "\tat java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:852)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:24 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:24 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:24 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:25 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:25 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:26 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:26 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:26 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:27 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:27 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:27 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:28 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:28 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:28 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:28 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:29 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:29 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:30 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:30 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:30 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:31 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:31 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:31 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:31 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:32 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:32 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:32 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:33 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:33 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:33 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker: nodename nor servname provided, or not known\n",
      "\tat java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:933)\n",
      "\tat java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543)\n",
      "\tat java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:852)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:34 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:34 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:34 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:35 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:35 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:35 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:36 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:36 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:36 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:37 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:37 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:37 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:38 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:38 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:38 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:39 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:39 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:39 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:40 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:40 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:40 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:41 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:41 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:42 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:42 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:42 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:43 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:43 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:44 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker: nodename nor servname provided, or not known\n",
      "\tat java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:933)\n",
      "\tat java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543)\n",
      "\tat java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:852)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:44 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:44 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:45 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:45 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:45 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:46 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:46 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:46 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:46 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:47 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:47 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:47 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:48 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:48 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:49 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:49 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:49 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:50 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:50 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:50 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:51 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:51 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:51 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:52 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:52 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:53 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:53 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:53 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:53 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:54 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker: nodename nor servname provided, or not known\n",
      "\tat java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:933)\n",
      "\tat java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543)\n",
      "\tat java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:852)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:54 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:54 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:55 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:55 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:55 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:56 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:56 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:56 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:57 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:57 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:57 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:58 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:58 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:58 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:59 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:59 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:31:59 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:00 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:00 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:00 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:01 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:01 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:01 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:02 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:02 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:02 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:03 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:03 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:03 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:04 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker: nodename nor servname provided, or not known\n",
      "\tat java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:933)\n",
      "\tat java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543)\n",
      "\tat java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:852)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:04 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:05 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:05 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:05 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:06 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:06 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:06 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:07 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:07 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:08 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:08 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:08 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:09 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:09 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:09 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0-4, groupId=spark-kafka-source-fa1f3aa3-3316-4705-9017-986fb18f0442--1938547164-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:09 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0-5, groupId=spark-kafka-source-9fbb1125-43f1-4179-89e4-a619845891d0--869611504-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/07/02 08:32:10 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0-6, groupId=spark-kafka-source-35cc4858-456d-4cfb-8bfd-4f9da25dbfb3--2104014241-driver-0] Error connecting to node broker:29092 (id: 1 rack: null)\n",
      "java.net.UnknownHostException: broker\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1532)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1384)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1305)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:600)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:599)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:567)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:536)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:298)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n"
     ]
    }
   ],
   "source": [
    "def write_mongo_row(df, epoch_id):\n",
    "    df.write\\\n",
    "        .format(\"mongodb\")\\\n",
    "        .mode(\"append\")\\\n",
    "        .option(\"uri\", mongoURL)\\\n",
    "        .save()\n",
    "    \n",
    "    \n",
    "    #com.mongodb.spark.sql.DefaultSource\n",
    "    pass\n",
    "\n",
    "liveData.writeStream.foreachBatch(write_mongo_row).start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "realtime-data-streaming-Za3P1rTN-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
